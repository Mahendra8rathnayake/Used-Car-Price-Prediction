


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix
from sklearn.metrics import mean_squared_error, r2_score



# IMPORT:
old_car = pd.read_csv('used_car_price_dataset_extended.csv')



old_car.head()



old_car.shape


old_car.select_dtypes(include='object').columns







for col in old_car.select_dtypes(include='object').columns:
    print(f"{col}: {old_car[col].nunique()} unique values")
    print(old_car[col].value_counts().head(), "\n")
    





old_car.info()         # Check datatypes and non-null counts



old_car.describe()     # Quick summary statistics






# See how many missing values
old_car.isnull().sum().sort_values(ascending=False)



old_car['service_history'].value_counts()






# Get all object (string) columns
object_columns = old_car.select_dtypes(include='object').columns

print("Object columns in dataset:")
print(object_columns)


old_car.columns





old_car.columns = old_car.columns.str.lower().str.strip()



old_car.columns


old_car['service_history']


old_car = old_car.dropna(subset=['service_history'])


old_car['service_history']







# Convert 'fuel_type' to integers
old_car['fuel_type'] = old_car['fuel_type'].map({
    'petrol': 1,
    'diesel': 2,
    'cng': 3,
    'electric': 4
}).fillna(0).astype(int)  # 0 for unknown/missing




old_car['fuel_type'].unique()


# Convert 'service_history' to integers
old_car['service_history'] = old_car['service_history'].map({
    'full': 1,
    'partial': 2
}).fillna(3).astype(int)  # 3 for missing




old_car['service_history'].unique()


# Convert 'insurance_valid' to integers
old_car['insurance_valid'] = old_car['insurance_valid'].map({
    'yes': 1,
    'no': 0
}).fillna(2).astype(int)  # 2 for missing


old_car['insurance_valid'].unique()


# See how many missing values
old_car.isnull().sum().sort_values(ascending=False)



# Select numeric columns only
old_car_numeric = old_car.select_dtypes(include=['int64', 'float64'])

# Show summary statistics
old_car_numeric.describe()





# Drop all duplicate rows
old_car = old_car.drop_duplicates()


old_car.duplicated().sum()





old_car.corr(numeric_only=True)[['price_usd']].sort_values(by='price_usd', ascending=False)


# Drop rows with Null values
old_car_clean = old_car.dropna()

# Correlation with price_usd
corr_price = (
    old_car_clean.corr(numeric_only=True)[['price_usd']]
    .sort_values(by='price_usd', ascending=False)
)

print(corr_price)


# Drop rows with Null values
old_car_clean = old_car.dropna()

# Correlation with price_usd
corr_price = (
    old_car_clean.corr(numeric_only=True)[['price_usd']]
    .sort_values(by='price_usd', ascending=False)
)

# Plot heatmap
plt.figure(figsize=(6, 10))
sns.heatmap(corr_price, annot=True, cmap="coolwarm", fmt=".2f", cbar=False)
plt.title("Correlation of Features with Price (USD)")
plt.show()








old_car['brand'].isna().sum()








sns.histplot(old_car['price_usd'], kde=True, bins=30, color='skyblue')
plt.title("Distribution of Car Prices")
plt.xlabel("Price (USD)")
plt.show()








plt.figure(figsize=(8, 8))

sns.boxplot(x='brand', y='price_usd', data=old_car)
plt.xticks(rotation=45)
plt.title("Price by Brand")
plt.show()








sns.boxplot(x='transmission', y='price_usd', data=old_car)
plt.title("Price by Transmission Type")
plt.show()








plt.figure(figsize=(10, 10))

sns.scatterplot(x='mileage_kmpl', y='price_usd', data=old_car, alpha=0.6)
plt.title("Mileage vs Price")
plt.show()








plt.figure(figsize=(8, 8))

old_car['car_age'] = 2025 - old_car['make_year']
sns.scatterplot(x='car_age', y='price_usd', data=old_car, alpha=0.6)
plt.title("Car Age vs Price")
plt.show()










sns.countplot(x='fuel_type', hue='brand', data=old_car, palette='Set2')
plt.title("Fuel Type Distribution by Brand")
plt.xticks(rotation=45)
plt.show()








numeric_cols = old_car.select_dtypes(include=['int64', 'float64']).columns
sns.heatmap(old_car[numeric_cols].corr(), annot=True, fmt=".2f", cmap='coolwarm')
plt.title("Correlation Heatmap of Numeric Features")
plt.show()








sns.pairplot(old_car[numeric_cols])
plt.suptitle("Pairplot of Numeric Features", y=1.02)
plt.show()








plt.figure(figsize=(6, 6))

brand_price = old_car.groupby('brand')['price_usd'].mean().sort_values()
brand_price.plot(kind='bar', color='lightgreen')
plt.title("Average Price by Brand")
plt.ylabel("Average Price (USD)")
plt.show()








plt.figure(figsize=(10, 6))

old_car['make_year'] = 2025 - old_car['make_year']
sns.boxplot(x='make_year', y='price_usd', data=old_car)
plt.xticks(rotation=45)
plt.title('Car Age vs Price')
plt.show()








# Setting up X (features) and y (target)
features = ['fuel_type', 'brand', 'color', 'service_history', 'insurance_valid']   # you can add more columns inside this list
X = old_car[features]
y = old_car['price_usd']


X


print(old_car.columns)



print(old_car.dtypes)


old_car.head()






categorical_cols = ['transmission', 'brand', 'color']

# Create dummy variables (drop_first=True avoids dummy trap)
encoded_cols = pd.get_dummies(old_car[categorical_cols], drop_first=True)

# Concatenate back to dataset
old_car = pd.concat([old_car, encoded_cols], axis=1)

# Drop original categorical columns (optional, since they’re encoded now)
old_car = old_car.drop(columns=categorical_cols)

# Check result
print(old_car.head())


# --- Visualization: Countplot example ---
plt.figure(figsize=(10, 5))
encoded_cols.sum().sort_values(ascending=False).plot(kind="bar", color="skyblue", edgecolor="black")
plt.title("Frequency of Encoded Categories")
plt.ylabel("Count")
plt.xlabel("Encoded Categories")
plt.show()





# Make sure car age column exists
old_car['car_age'] = 2025 - old_car['make_year']

# Create a pivot table: car_age vs average price
pivot_table = old_car.pivot_table(index='car_age', 
                                  values='price_usd', 
                                  aggfunc='mean')

# Plot the heatmap
plt.figure(figsize=(10,8))
sns.heatmap(pivot_table, annot=True, fmt=".0f", cmap="YlGnBu")
plt.title("Average Car Price vs Car Age")
plt.ylabel("Car Age")
plt.xlabel("Average Price (USD)")
plt.show()



print(old_car.columns.tolist())


# Count column types
dtype_counts = old_car.dtypes.value_counts()

# Plot
dtype_counts.plot(kind='barh', color='skyblue', edgecolor='black')
plt.title("Number of Columns by Data Type")
plt.xlabel("Count")
plt.ylabel("Data Type")
plt.show();





# Mean baseline → always predict the average car price.

baseline_mean = old_car['price_usd'].mean()
baseline_mean


# Median baseline → always predict the median car price (more robust to outliers).
baseline_median = old_car['price_usd'].median()
baseline_median











from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score


# 1. Select only numerical features
numeric_cols = old_car.select_dtypes(include=['int64', 'float64']).columns


# 2. Define X (features) and y (target)
X = old_car[numeric_cols].drop(columns=['price_usd'], errors='ignore')
y = old_car['price_usd']





# 3. Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# 4. Initialize and fit the model
lr = LinearRegression()
lr.fit(X_train, y_train)


# 5. Predictions
y_pred = lr.predict(X_test)


# 6. Evaluate model
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)


print("Mean Squared Error:", mse)
print("R² Score:", r2)









# Convert continuous price into 2 classes
old_car['price_category'] = (old_car['price_usd'] > old_car['price_usd'].median()).astype(int)


# Use this as target y
y = old_car['price_category']

print(old_car[['price_usd', 'price_category']].head())


# Convert continuous price into 2 classes
old_car['price_category'] = (old_car['price_usd'] > old_car['price_usd'].median()).astype(int)

# Use this as y
y = old_car['price_category']


categorical_cols = [col for col in ['transmission', 'brand', 'color', 'fuel_type', 'insurance_valid', 'service_history'] if col in old_car.columns]

for col in categorical_cols:
    old_car[col] = old_car[col].astype('category').cat.codes





# Split features and target
X = old_car.drop(columns=['price_usd', 'price_category'])
y = old_car['price_category']


# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


# Scale numeric features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve
import matplotlib.pyplot as plt


# 2. Initialize the model
logreg = LogisticRegression()


# 3. Fit the model on scaled training data
logreg.fit(X_train_scaled, y_train)





# 5. Evaluate
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))







# 6. Optional: ROC Curve
fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob[:,1])
roc_auc = auc(fpr, tpr)
plt.figure(figsize=(6,6))
plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')
plt.plot([0,1], [0,1], 'k--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()











# 7. Check feature coefficients
coefficients = pd.DataFrame({
    "Feature": X.columns,
    "Coefficient": lr.coef_
}).sort_values(by="Coefficient", ascending=False)

print(coefficients)





# Plot coefficients (Recreate coefficients DataFrame)
plt.figure(figsize=(10, 6))
sns.barplot(
    x="Coefficient",
    y="Feature",
    data=coefficients,
    hue="Feature",        
    dodge=False,
    legend=False,
    palette="coolwarm"
)
plt.title("Linear Regression Coefficients (Feature Importance)")
plt.xlabel("Coefficient Value")
plt.ylabel("Feature")
plt.axvline(x=0, color="black", linestyle="--")  # baseline at 0
plt.show()








# 1. Select only numerical features
numeric_cols = old_car.select_dtypes(include=['int64', 'float64']).columns
X = old_car[numeric_cols].drop(columns=['price_usd'], errors='ignore')
y = old_car['price_usd']





# Split dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y,
    test_size=0.2,      # 20% for testing
    random_state=42     # reproducibility
)


# StandardScaler: 

sc = StandardScaler()


# Fit and Transform the Train set:

X_train_sc = sc.fit_transform(X_train)



# Transform the Test set: 

X_test_sc = sc.transform(X_test)








# Instantiate KNN for regression
knn = KNeighborsRegressor(n_neighbors=5)


# Fit the model
knn.fit(X_train, y_train)


# 4. Predictions
y_pred_knn = knn.predict(X_test)


# 5. Evaluate model
mse_knn = mean_squared_error(y_test, y_pred_knn)
r2_knn = r2_score(y_test, y_pred_knn)


print("KNN Mean Squared Error:", mse_knn)
print("KNN R² Score:", r2_knn)


# 6. Compare actual vs predicted
plt.figure(figsize=(8,6))
plt.scatter(y_test, y_pred_knn, alpha=0.6, color="blue")
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)  # perfect prediction line
plt.xlabel("Actual Price (USD)")
plt.ylabel("Predicted Price (USD)")
plt.title("KNN Regression: Actual vs Predicted")
plt.show()








# 1. Select only numeric columns
numeric_cols = old_car.select_dtypes(include=['int64', 'float64']).columns




# 2. Define features (X) and target (y)
X = old_car[numeric_cols].drop(columns=['price_usd'], errors='ignore')
y = old_car['price_usd']


# 3. Train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


# 4. Initialize Random Forest model
rf = RandomForestRegressor(
    n_estimators=100,     # number of trees
    random_state=42,
    n_jobs=-1            # use all CPU cores
)


# 5. Train the model
rf.fit(X_train, y_train)


# 6. Predictions
y_pred_rf = rf.predict(X_test)



# 7. Evaluate model
mse = mean_squared_error(y_test, y_pred_rf)
r2 = r2_score(y_test, y_pred_rf)


print("Random Forest Mean Squared Error:", mse)
print("Random Forest R² Score:", r2)


# 8. Feature importance visualization
feature_importance = pd.DataFrame({
    "Feature": X.columns,
    "Importance": rf.feature_importances_
}).sort_values(by="Importance", ascending=False)


plt.figure(figsize=(10, 6))
plt.barh(feature_importance["Feature"], feature_importance["Importance"], color="skyblue")
plt.gca().invert_yaxis()
plt.title("Feature Importance from Random Forest")
plt.xlabel("Importance")
plt.show()








from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score


# Numeric features
numeric_cols = old_car.select_dtypes(include=['int64', 'float64']).columns
X = old_car[numeric_cols].drop(columns=['price_usd'], errors='ignore')
y = old_car['price_usd']


# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)



# Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# Define MLP model
mlp = MLPRegressor(hidden_layer_sizes=(64,32), activation='relu', max_iter=500, random_state=42)


# Fit model
mlp.fit(X_train_scaled, y_train)


# Predict
y_pred = mlp.predict(X_test_scaled)


# Evaluate
print("MSE:", mean_squared_error(y_test, y_pred))
print("R2:", r2_score(y_test, y_pred))


plt.figure(figsize=(8,6))
plt.scatter(y_test, y_pred, alpha=0.6, color='blue')
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'k--', lw=2)  # perfect prediction line
plt.xlabel("Actual Price (USD)")
plt.ylabel("Predicted Price (USD)")
plt.title("MLP Neural Network: Actual vs Predicted")
plt.show()









